{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原始模型\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
    "from tqdm import tqdm,trange\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "classes = 5  #分类\n",
    "hidden_dim = 512 # rnn隐藏单元数\n",
    "lr = 0.001 # 学习率\n",
    "epoches = 50 #训练次数\n",
    "batch_size = 16 # 每一个训练批次数量\n",
    "input_dim= 54\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionDatasets(Dataset):\n",
    "    def __init__(self, csv_path, transform=None, target_transform=None, pick_path = \"data.npz\"):\n",
    "        super(ActionDatasets, self).__init__()\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        import pandas as pd\n",
    "        from glob import glob\n",
    "        import os\n",
    "        csvs = glob(os.path.join(csv_path, \"*.csv\"))\n",
    "        csvs.sort()\n",
    "        if len(csvs) == 0:\n",
    "            raise ValueError(\"路径下不存在csv文件\")\n",
    "        df = []\n",
    "        labels = []\n",
    "        if os.path.exists(pick_path):\n",
    "            load_data = np.load(pick_path)\n",
    "            values = load_data['x']\n",
    "            labels = load_data['y']\n",
    "        else:\n",
    "            for csv in csvs:\n",
    "                label = ord(csv.split('\\\\')[-1][2]) - 65\n",
    "                if label >= 4:\n",
    "                    label -= 1\n",
    "                labels.append(label)\n",
    "                if type(df) == list:\n",
    "                    df = pd.read_csv(csv)\n",
    "                else:\n",
    "                    df_tmp = pd.read_csv(csv)\n",
    "                    df = pd.concat([df, df_tmp])\n",
    "        \n",
    "            values = df.values[:, 1:]\n",
    "            labels = np.array(labels)\n",
    "            np.savez(pick_path, x=values, y=labels)\n",
    "            \n",
    "        self.label = labels\n",
    "        self.values = values\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        train_data, label_data = torch.tensor(self.values[idx*140:(idx+1)*140, :],dtype=torch.float32) ,torch.tensor(self.label[idx], dtype=torch.long)\n",
    "\n",
    "        if self.transform:\n",
    "            train_data = self.transform(train_data)\n",
    "        if self.target_transform:\n",
    "            label_data = self.target_transform(label_data)\n",
    "\n",
    "        return train_data ,label_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.values.shape[0]//140\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "训练集大小847， 验证集大小212， 测试集大小553\n"
     ]
    }
   ],
   "source": [
    "def create_data_loader():\n",
    "    train_data_path = \"./action_train/\"\n",
    "    # train_data_path = \"D:\\\\temp\\\\augment_action_windows\"\n",
    "    datasets = ActionDatasets(train_data_path, transform=torch.tensor, target_transform=torch.tensor, pick_path='train.npz')\n",
    "    test_datasets = ActionDatasets(\"./action_test/\", transform=torch.tensor, target_transform=torch.tensor, pick_path='test.npz')\n",
    "    split_rate = 0.8  # 训练集占整个数据集的比例\n",
    "    train_len = int(split_rate * len(datasets))\n",
    "    valid_len = len(datasets) - train_len\n",
    "\n",
    "    train_sets, valid_sets = random_split(datasets, [train_len, valid_len])\n",
    "\n",
    "    train_loader = DataLoader(train_sets, batch_size=batch_size, shuffle=True,drop_last=True,pin_memory=True)\n",
    "    test_loader = DataLoader(test_datasets, batch_size=batch_size, shuffle=True,drop_last=True,pin_memory=True)\n",
    "    valid_loader = DataLoader(valid_sets, batch_size=batch_size, shuffle=True,drop_last=True,pin_memory=True)\n",
    "\n",
    "    print(f\"训练集大小{len(train_sets)}， 验证集大小{len(valid_sets)}， 测试集大小{len(test_datasets)}\")\n",
    "    return train_loader, valid_loader, test_loader\n",
    "train_loader, valid_loader, test_loader = create_data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([16, 140, 54])\ntensor([0, 3, 1, 0, 3, 1, 1, 4, 1, 0, 0, 2, 4, 0, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "for data, label in train_loader:\n",
    "    print(data.shape)\n",
    "    print(label)\n",
    "    break"
   ]
  },
  {
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, out_dim):\n",
    "        super(RNN, self).__init__()\n",
    "        self.pre = nn.Linear(input_dim, hidden_dim)\n",
    "        self.rnn = nn.LSTM(hidden_dim, hidden_dim, 1, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, out_dim)\n",
    "    def forward(self, X):\n",
    "        X = self.pre(X)\n",
    "        out, status = self.rnn(X)\n",
    "        out = self.linear(out[:, -1, :])\n",
    "        return out"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(input_dim, hidden_dim, classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GETACC(loader=valid_loader, typ_='valid'):\n",
    "    rnn.eval()\n",
    "    cnt = 0\n",
    "    sum_valid_acc = 0\n",
    "    sum_valid_loss = 0\n",
    "    c = [0]*5 #action4 被识别为其他动作的数量\n",
    "    if typ_ == 'test':\n",
    "        errors = [0]*classes # 识别错动作的数量\n",
    "    for data, label in loader:\n",
    "        data = data.detach().to(device)\n",
    "        label = label.detach().to(device)\n",
    "        out = rnn(data)\n",
    "        _, predict = torch.max(F.softmax(out), 1)\n",
    "        loss = criterion(out.detach(), label)\n",
    "        sum_valid_loss += loss.item()\n",
    "        \n",
    "        eq = (predict == label).int()\n",
    "        \n",
    "        if typ_ == 'test':\n",
    "            for p, l in zip(predict, label):\n",
    "                p = int(p.item())\n",
    "                l = int(l.item())\n",
    "                if p!=l : errors[l] += 1\n",
    "                if l==3:\n",
    "                    c[p]+=1\n",
    "        acc = torch.sum(eq).item() / batch_size\n",
    "        sum_valid_acc += acc\n",
    "        cnt+=1\n",
    "    if typ_ == 'test':\n",
    "        return sum_valid_loss/cnt, sum_valid_acc/cnt, errors, c\n",
    "    return sum_valid_loss/cnt, sum_valid_acc/cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "epoch = 0 train_loss = 1.368 valid_loss = 1.361 valid_acc = 32.692%: 100%|██████████| 52/52 [00:01<00:00, 30.72it/s]\n",
      "epoch = 1 train_loss = 0.954 valid_loss = 0.778 valid_acc = 59.615%: 100%|██████████| 52/52 [00:00<00:00, 67.43it/s]\n",
      "epoch = 2 train_loss = 0.815 valid_loss = 0.709 valid_acc = 71.635%: 100%|██████████| 52/52 [00:00<00:00, 66.50it/s]\n",
      "epoch = 3 train_loss = 0.808 valid_loss = 0.934 valid_acc = 54.808%: 100%|██████████| 52/52 [00:00<00:00, 67.41it/s]\n",
      "epoch = 4 train_loss = 0.749 valid_loss = 0.580 valid_acc = 72.596%: 100%|██████████| 52/52 [00:00<00:00, 67.54it/s]\n",
      "epoch = 5 train_loss = 0.593 valid_loss = 0.567 valid_acc = 73.077%: 100%|██████████| 52/52 [00:00<00:00, 66.33it/s]\n",
      "epoch = 6 train_loss = 0.666 valid_loss = 0.478 valid_acc = 83.654%: 100%|██████████| 52/52 [00:00<00:00, 66.89it/s]\n",
      "epoch = 7 train_loss = 0.582 valid_loss = 0.496 valid_acc = 71.154%: 100%|██████████| 52/52 [00:00<00:00, 66.93it/s]\n",
      "epoch = 8 train_loss = 0.731 valid_loss = 0.468 valid_acc = 84.135%: 100%|██████████| 52/52 [00:00<00:00, 67.62it/s]\n",
      "epoch = 9 train_loss = 0.857 valid_loss = 0.815 valid_acc = 56.731%: 100%|██████████| 52/52 [00:00<00:00, 67.15it/s]\n",
      "epoch = 10 train_loss = 0.802 valid_loss = 0.912 valid_acc = 50.481%: 100%|██████████| 52/52 [00:00<00:00, 67.36it/s]\n",
      "epoch = 11 train_loss = 0.831 valid_loss = 0.704 valid_acc = 66.346%: 100%|██████████| 52/52 [00:00<00:00, 67.15it/s]\n",
      "epoch = 12 train_loss = 0.719 valid_loss = 1.123 valid_acc = 50.000%: 100%|██████████| 52/52 [00:00<00:00, 67.10it/s]\n",
      "epoch = 13 train_loss = 0.669 valid_loss = 0.544 valid_acc = 71.635%: 100%|██████████| 52/52 [00:00<00:00, 66.25it/s]\n",
      "epoch = 14 train_loss = 0.475 valid_loss = 0.405 valid_acc = 87.981%: 100%|██████████| 52/52 [00:00<00:00, 67.32it/s]\n",
      "epoch = 15 train_loss = 0.415 valid_loss = 0.332 valid_acc = 85.577%: 100%|██████████| 52/52 [00:00<00:00, 65.54it/s]\n",
      "epoch = 16 train_loss = 0.712 valid_loss = 0.533 valid_acc = 70.192%: 100%|██████████| 52/52 [00:00<00:00, 66.28it/s]\n",
      "epoch = 17 train_loss = 0.447 valid_loss = 0.337 valid_acc = 91.346%: 100%|██████████| 52/52 [00:00<00:00, 66.67it/s]\n",
      "epoch = 18 train_loss = 0.391 valid_loss = 0.400 valid_acc = 82.692%: 100%|██████████| 52/52 [00:00<00:00, 65.16it/s]\n",
      "epoch = 19 train_loss = 0.295 valid_loss = 0.237 valid_acc = 91.827%: 100%|██████████| 52/52 [00:00<00:00, 64.65it/s]\n",
      "epoch = 20 train_loss = 0.233 valid_loss = 0.278 valid_acc = 91.827%: 100%|██████████| 52/52 [00:00<00:00, 65.50it/s]\n",
      "epoch = 21 train_loss = 0.212 valid_loss = 0.224 valid_acc = 91.346%: 100%|██████████| 52/52 [00:00<00:00, 64.13it/s]\n",
      "epoch = 22 train_loss = 0.072 valid_loss = 0.110 valid_acc = 96.635%: 100%|██████████| 52/52 [00:00<00:00, 67.67it/s]\n",
      "epoch = 23 train_loss = 0.173 valid_loss = 0.123 valid_acc = 97.115%: 100%|██████████| 52/52 [00:00<00:00, 66.12it/s]\n",
      "epoch = 24 train_loss = 0.096 valid_loss = 0.161 valid_acc = 94.712%: 100%|██████████| 52/52 [00:00<00:00, 65.62it/s]\n",
      "epoch = 25 train_loss = 0.052 valid_loss = 0.064 valid_acc = 98.077%: 100%|██████████| 52/52 [00:00<00:00, 65.17it/s]\n",
      "epoch = 26 train_loss = 0.031 valid_loss = 0.071 valid_acc = 98.558%: 100%|██████████| 52/52 [00:00<00:00, 66.12it/s]\n",
      "epoch = 27 train_loss = 0.361 valid_loss = 0.265 valid_acc = 91.827%: 100%|██████████| 52/52 [00:00<00:00, 65.01it/s]\n",
      "epoch = 28 train_loss = 0.116 valid_loss = 0.094 valid_acc = 98.077%: 100%|██████████| 52/52 [00:00<00:00, 64.50it/s]\n",
      "epoch = 29 train_loss = 0.188 valid_loss = 0.097 valid_acc = 99.038%: 100%|██████████| 52/52 [00:00<00:00, 64.17it/s]\n",
      "epoch = 30 train_loss = 0.085 valid_loss = 0.151 valid_acc = 93.750%: 100%|██████████| 52/52 [00:00<00:00, 65.70it/s]\n",
      "epoch = 31 train_loss = 0.033 valid_loss = 0.059 valid_acc = 99.038%: 100%|██████████| 52/52 [00:00<00:00, 64.97it/s]\n",
      "epoch = 32 train_loss = 0.011 valid_loss = 0.069 valid_acc = 98.558%: 100%|██████████| 52/52 [00:00<00:00, 65.53it/s]\n",
      "epoch = 33 train_loss = 0.057 valid_loss = 0.130 valid_acc = 97.115%: 100%|██████████| 52/52 [00:00<00:00, 65.79it/s]\n",
      "epoch = 34 train_loss = 0.021 valid_loss = 0.051 valid_acc = 98.558%: 100%|██████████| 52/52 [00:00<00:00, 65.30it/s]\n",
      "epoch = 35 train_loss = 0.007 valid_loss = 0.093 valid_acc = 98.558%: 100%|██████████| 52/52 [00:00<00:00, 65.13it/s]\n",
      "epoch = 36 train_loss = 0.021 valid_loss = 0.024 valid_acc = 98.558%: 100%|██████████| 52/52 [00:00<00:00, 67.41it/s]\n",
      "epoch = 37 train_loss = 0.017 valid_loss = 0.072 valid_acc = 98.077%: 100%|██████████| 52/52 [00:00<00:00, 68.05it/s]\n",
      "epoch = 38 train_loss = 0.004 valid_loss = 0.063 valid_acc = 98.558%: 100%|██████████| 52/52 [00:00<00:00, 67.67it/s]\n",
      "epoch = 39 train_loss = 0.003 valid_loss = 0.062 valid_acc = 99.038%: 100%|██████████| 52/52 [00:00<00:00, 67.32it/s]\n",
      "epoch = 40 train_loss = 0.002 valid_loss = 0.065 valid_acc = 99.038%: 100%|██████████| 52/52 [00:00<00:00, 67.89it/s]\n",
      "epoch = 41 train_loss = 0.002 valid_loss = 0.064 valid_acc = 99.038%: 100%|██████████| 52/52 [00:00<00:00, 67.89it/s]\n",
      "epoch = 42 train_loss = 0.001 valid_loss = 0.068 valid_acc = 99.038%: 100%|██████████| 52/52 [00:00<00:00, 67.89it/s]\n",
      "epoch = 43 train_loss = 0.001 valid_loss = 0.070 valid_acc = 99.038%: 100%|██████████| 52/52 [00:00<00:00, 67.38it/s]\n",
      "epoch = 44 train_loss = 0.001 valid_loss = 0.071 valid_acc = 99.038%: 100%|██████████| 52/52 [00:00<00:00, 68.02it/s]\n",
      "epoch = 45 train_loss = 0.001 valid_loss = 0.073 valid_acc = 99.038%: 100%|██████████| 52/52 [00:00<00:00, 68.36it/s]\n",
      "epoch = 46 train_loss = 0.001 valid_loss = 0.073 valid_acc = 99.038%: 100%|██████████| 52/52 [00:00<00:00, 67.62it/s]\n",
      "epoch = 47 train_loss = 0.001 valid_loss = 0.075 valid_acc = 99.038%: 100%|██████████| 52/52 [00:00<00:00, 67.62it/s]\n",
      "epoch = 48 train_loss = 0.001 valid_loss = 0.076 valid_acc = 99.038%: 100%|██████████| 52/52 [00:00<00:00, 68.47it/s]\n",
      "epoch = 49 train_loss = 0.001 valid_loss = 0.077 valid_acc = 99.038%: 100%|██████████| 52/52 [00:00<00:00, 67.32it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoches):\n",
    "    i = 0\n",
    "    loss_sum = 0\n",
    "    bar = tqdm(train_loader)\n",
    "    for ii, (data , label) in enumerate(bar):\n",
    "        rnn.train()\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "        out = rnn(data)\n",
    "        loss = criterion(out, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        i+=1\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        if ii == len(train_loader)-1:\n",
    "            valid_loss, valid_acc = GETACC(valid_loader)\n",
    "            \n",
    "            bar.set_description(\"epoch = {} train_loss = {:.3f} valid_loss = {:.3f} valid_acc = {:.3f}%\".format(epoch, loss_sum/i, valid_loss,valid_acc*100))\n",
    "        # test_loss,test_acc, errors = GETACC(test_loader, 'test')\n",
    "        # print(\"test_loss = {:.3f}, test_acc = {:.3f}%, error_action = {}\".format(test_loss, test_acc*100, errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test_loss = 0.0004096484823522213, test_acc = 1.0\n"
     ]
    }
   ],
   "source": [
    "test_loss,test_acc, errors, c = GETACC(test_loader, 'test')\n",
    "print(f\"test_loss = {test_loss}, test_acc = {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}